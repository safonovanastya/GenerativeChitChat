{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "exposed-skirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from typing import Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from nltk import tokenize\n",
    "import youtokentome as yttm\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-chinese",
   "metadata": {},
   "source": [
    "# Создаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "controversial-physics",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "innocent-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path):\n",
    "    data=[]\n",
    "    with open(path, 'r', encoding='utf-8') as reader:\n",
    "        for line in reader:\n",
    "            data.append(json.loads(line))\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "emerging-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_data = load_jsonl('qa_data.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "understood-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for line in qa_data:\n",
    "    for response in line['responses']:\n",
    "        all_data.append(\n",
    "            {'question': line['question'].lower(),\n",
    "             'response': response.lower()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "behind-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.json_normalize(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "athletic-tragedy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7767138\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>долго ли идут деньги с яндексденег на карту visa?</td>\n",
       "      <td>нет. прорыв 35 ;)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>можно ли зарегистрировать авто в другом регионе</td>\n",
       "      <td>можно на родственника из того региона.. .  а п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>что делать если у меня очень тонкие ногти а хо...</td>\n",
       "      <td>витамины и умная эмаль (каждый день)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>что делать если у меня очень тонкие ногти а хо...</td>\n",
       "      <td>ванночки с морской солью. с вечера мажь ногти ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>что делать если у меня очень тонкие ногти а хо...</td>\n",
       "      <td>умная эмаль, витамины, йод, и поменьше крась л...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  долго ли идут деньги с яндексденег на карту visa?   \n",
       "1    можно ли зарегистрировать авто в другом регионе   \n",
       "2  что делать если у меня очень тонкие ногти а хо...   \n",
       "3  что делать если у меня очень тонкие ногти а хо...   \n",
       "4  что делать если у меня очень тонкие ногти а хо...   \n",
       "\n",
       "                                            response  \n",
       "0                                  нет. прорыв 35 ;)  \n",
       "1  можно на родственника из того региона.. .  а п...  \n",
       "2               витамины и умная эмаль (каждый день)  \n",
       "3  ванночки с морской солью. с вечера мажь ногти ...  \n",
       "4  умная эмаль, витамины, йод, и поменьше крась л...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "metropolitan-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('for_bpe.txt', 'w', encoding='utf-8') as f:\n",
    "    for res in data.response:\n",
    "        f.write(res + '\\n')\n",
    "        \n",
    "# параметры\n",
    "vocab_size = 30_000\n",
    "model_path = 'pretrained_bpe_lm.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dangerous-paper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<youtokentome.youtokentome.BPE at 0x7f464f8ca730>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yttm.BPE.train(data='for_bpe.txt', vocab_size=vocab_size, model=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sticky-career",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<PAD>',\n",
       " '<UNK>',\n",
       " '<BOS>',\n",
       " '<EOS>',\n",
       " '▁',\n",
       " 'о',\n",
       " 'е',\n",
       " 'а',\n",
       " 'т',\n",
       " 'н',\n",
       " 'и',\n",
       " 'с',\n",
       " 'р',\n",
       " 'в',\n",
       " 'л']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загружаем токенизатор\n",
    "tokenizer = yttm.BPE(model=model_path)\n",
    "\n",
    "tokenizer.vocab()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "worth-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = data.sample(frac=0.005, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "relative-relative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38836\n"
     ]
    }
   ],
   "source": [
    "print(len(small_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "wooden-multimedia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3194253</th>\n",
       "      <td>а вот у меня нет любви...я ценю свободу...а вы...</td>\n",
       "      <td>иногда хочется какой-то определенности но в ос...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5451557</th>\n",
       "      <td>подскажите название телепередачи!!!</td>\n",
       "      <td>бля кажется по дисковери было или експлоуер)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314992</th>\n",
       "      <td>дамы! что есть хуже порванных новых колготок, ...</td>\n",
       "      <td>ты порвал свои новые колготки прямо перед выхо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7522285</th>\n",
       "      <td>бухгалтера салона красоты отзовитесь, пожалуйста!</td>\n",
       "      <td>для услуг я бы использовала 26 но можно и на 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000707</th>\n",
       "      <td>за сколько времени зарядится акб на 500mah. ес...</td>\n",
       "      <td>от силы тока зарядки зависит</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  question  \\\n",
       "3194253  а вот у меня нет любви...я ценю свободу...а вы...   \n",
       "5451557                подскажите название телепередачи!!!   \n",
       "3314992  дамы! что есть хуже порванных новых колготок, ...   \n",
       "7522285  бухгалтера салона красоты отзовитесь, пожалуйста!   \n",
       "6000707  за сколько времени зарядится акб на 500mah. ес...   \n",
       "\n",
       "                                                  response  \n",
       "3194253  иногда хочется какой-то определенности но в ос...  \n",
       "5451557       бля кажется по дисковери было или експлоуер)  \n",
       "3314992  ты порвал свои новые колготки прямо перед выхо...  \n",
       "7522285  для услуг я бы использовала 26 но можно и на 2...  \n",
       "6000707                       от силы тока зарядки зависит  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "competent-egypt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31068, 1)\n",
      "(31068,)\n",
      "(3883, 1)\n",
      "(3883,)\n",
      "(3885, 1)\n",
      "(3885,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "valid_size=0.1\n",
    "\n",
    "train_index = int(len(small_data)*train_size)\n",
    "\n",
    "df_train = small_data[0:train_index]\n",
    "df_rem = small_data[train_index:]\n",
    "\n",
    "valid_index = int(len(small_data)*valid_size)\n",
    "\n",
    "df_valid = small_data[train_index:train_index+valid_index]\n",
    "df_test = small_data[train_index+valid_index:]\n",
    "\n",
    "X_train, y_train = df_train.drop(columns='response').copy(), df_train['response'].copy()\n",
    "X_valid, y_valid = df_valid.drop(columns='response').copy(), df_valid['response'].copy()\n",
    "X_test, y_test = df_test.drop(columns='response').copy(), df_test['response'].copy()\n",
    "        \n",
    "print(X_train.shape), print(y_train.shape)\n",
    "print(X_valid.shape), print(y_valid.shape)\n",
    "print(X_test.shape), print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "every-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.encode(X_train.question.to_list(), bos=True)\n",
    "y_train = tokenizer.encode(y_train.to_list(), bos=True)\n",
    "X_valid = tokenizer.encode(X_valid.question.to_list(), bos=True)\n",
    "y_valid = tokenizer.encode(y_valid.to_list(), bos=True)\n",
    "X_test = tokenizer.encode(X_test.question.to_list(), bos=True)\n",
    "y_test = tokenizer.encode(y_test.to_list(), bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "partial-output",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, seq_1, seq_2, max_len, pad_index):\n",
    "        \n",
    "        self.questions = seq_1\n",
    "        self.responses = seq_2\n",
    "        self.max_len = max_len\n",
    "        self.pad_index = pad_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        questions = self.questions[index][:self.max_len]\n",
    "        responses = self.responses[index][:self.max_len]\n",
    "        \n",
    "        \n",
    "        q_pads = [self.pad_index] * (self.max_len - len(questions))\n",
    "        r_pads = [self.pad_index] * (self.max_len - len(responses))\n",
    "        \n",
    "        questions = torch.tensor(questions + q_pads).long().view(-1, 1)\n",
    "        responses = torch.tensor(responses + r_pads).long().view(-1, 1)\n",
    "        \n",
    "        return questions, responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "verbal-grammar",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SeqDataset(seq_1=list(X_train), seq_2=list(y_train), max_len = 30, pad_index = 0)\n",
    "valid_dataset = SeqDataset(seq_1=list(X_valid), seq_2=list(y_valid), max_len = 30, pad_index = 0)\n",
    "test_dataset = SeqDataset(seq_1=list(X_test), seq_2=list(y_test), max_len = 30, pad_index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "human-appearance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SeqDataset at 0x7f43bdcafd60>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "heavy-theater",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 30, 1]), torch.Size([64, 30, 1]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=64)\n",
    "\n",
    "for x, y in train_loader:\n",
    "    break\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-curtis",
   "metadata": {},
   "source": [
    "# Делаем модель"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-continuity",
   "metadata": {},
   "source": [
    "Отталкиваюсь от статьи 'Convolutional Sequence to Sequence Learning': https://arxiv.org/abs/1705.03122\n",
    "\n",
    "Пока не закончено"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "absent-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 emb_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 kernel_size, \n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert kernel_size % 2 == 1, \"Kernel size must be odd!\"\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
    "        \n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
    "                                              out_channels = 2 * hid_dim, \n",
    "                                              kernel_size = kernel_size, \n",
    "                                              padding = (kernel_size - 1) // 2)\n",
    "                                    for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        tok_embedded = self.tok_embedding(src)\n",
    "        pos_embedded = self.pos_embedding(pos)\n",
    "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
    "        conv_input = self.emb2hid(embedded)\n",
    "        conv_input = conv_input.permute(0, 2, 1) \n",
    "\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "\n",
    "            conved = conv(self.dropout(conv_input))\n",
    "            conved = F.glu(conved, dim = 1)\n",
    "            conved = (conved + conv_input) * self.scale\n",
    "            conv_input = conved\n",
    "        \n",
    "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
    "        combined = (conved + embedded) * self.scale\n",
    "        \n",
    "        \n",
    "        return conved, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "regulated-syntax",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 output_dim, \n",
    "                 emb_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 kernel_size, \n",
    "                 dropout, \n",
    "                 trg_pad_idx, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
    "        \n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        \n",
    "        self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
    "                                              out_channels = 2 * hid_dim, \n",
    "                                              kernel_size = kernel_size)\n",
    "                                    for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "      \n",
    "    def calculate_attention(self, embedded, conved, encoder_conved, encoder_combined):\n",
    "\n",
    "        conved_emb = self.attn_hid2emb(conved.permute(0, 2, 1))\n",
    "        combined = (conved_emb + embedded) * self.scale\n",
    "                \n",
    "        energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))\n",
    "        \n",
    "        attention = F.softmax(energy, dim=2)\n",
    "        attended_encoding = torch.matmul(attention, encoder_combined)\n",
    "        attended_encoding = self.attn_emb2hid(attended_encoding)\n",
    "        attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale\n",
    "        \n",
    "        \n",
    "        return attention, attended_combined\n",
    "        \n",
    "    def forward(self, trg, encoder_conved, encoder_combined):\n",
    "                \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "            \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        tok_embedded = self.tok_embedding(trg)\n",
    "        pos_embedded = self.pos_embedding(pos)\n",
    "        \n",
    "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
    "        \n",
    "        conv_input = self.emb2hid(embedded)\n",
    "        \n",
    "        conv_input = conv_input.permute(0, 2, 1) \n",
    "        \n",
    "        \n",
    "        batch_size = conv_input.shape[0]\n",
    "        hid_dim = conv_input.shape[1]\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "        \n",
    "            conv_input = self.dropout(conv_input)\n",
    "        \n",
    "            padding = torch.zeros(batch_size, \n",
    "                                  hid_dim, \n",
    "                                  self.kernel_size - 1).fill_(self.trg_pad_idx).to(self.device)\n",
    "                \n",
    "            padded_conv_input = torch.cat((padding, conv_input), dim = 2)\n",
    "\n",
    "            conved = conv(padded_conv_input)\n",
    "\n",
    "            conved = F.glu(conved, dim = 1)\n",
    "\n",
    "            attention, conved = self.calculate_attention(embedded, \n",
    "                                                         conved, \n",
    "                                                         encoder_conved, \n",
    "                                                         encoder_combined)\n",
    "\n",
    "            conved = (conved + conv_input) * self.scale\n",
    "            \n",
    "            conv_input = conved\n",
    "            \n",
    "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
    "         \n",
    "            \n",
    "        output = self.fc_out(self.dropout(conved))\n",
    "            \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "mysterious-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, src, trg):\n",
    "\n",
    "        encoder_conved, encoder_combined = self.encoder(src)\n",
    "        output, attention = self.decoder(trg, encoder_conved, encoder_combined)\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "muslim-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(tokenizer.vocab())\n",
    "OUTPUT_DIM = len(tokenizer.vocab())\n",
    "EMB_DIM = 256\n",
    "HID_DIM = 512 # each conv. layer has 2 * hid_dim filters\n",
    "ENC_LAYERS = 10 # number of conv. blocks in encoder\n",
    "DEC_LAYERS = 10 # number of conv. blocks in decoder\n",
    "ENC_KERNEL_SIZE = 3 # must be odd!\n",
    "DEC_KERNEL_SIZE = 3 # can be even or odd\n",
    "ENC_DROPOUT = 0.25\n",
    "DEC_DROPOUT = 0.25\n",
    "TRG_PAD_IDX = 0\n",
    "    \n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, ENC_LAYERS, ENC_KERNEL_SIZE, ENC_DROPOUT, device)\n",
    "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DEC_LAYERS, DEC_KERNEL_SIZE, DEC_DROPOUT, TRG_PAD_IDX, device)\n",
    "\n",
    "model = Seq2Seq(enc, dec).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "geological-antenna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 55,387,696 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "conservative-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "loaded-jaguar",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "respected-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-antarctica",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-binding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 0.1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut5-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
